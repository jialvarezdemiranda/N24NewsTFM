{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, CLIPProcessor, AutoConfig, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from PIL import Image\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "SEED=1234542\n",
    "\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "\n",
    "df_train=pd.read_csv('../../data/splitted/train.csv')\n",
    "df_validation=pd.read_csv('../../data/splitted/validation.csv')\n",
    "df_test=pd.read_csv('../../data/splitted/test.csv')\n",
    "\n",
    "# Remove nan from caption column\n",
    "df_train.fillna(value=\"\", inplace=True)\n",
    "df_validation.fillna(value=\"\", inplace=True)\n",
    "df_test.fillna(value=\"\", inplace=True)\n",
    "\n",
    "label_dict={0: 'Movies', 1: 'Sports', 2: 'Music', 3: 'Opinion', 4: 'Media', 5: 'Art & Design', 6: 'Theater', 7: 'Television', 8: 'Technology', 9: 'Economy', 10: 'Books', 11: 'Style', 12: 'Travel', 13: 'Health', 14: 'Real Estate', 15: 'Dance', 16: 'Science', 17: 'Fashion', 18: 'Well', 19: 'Food', 20: 'Your Money', 21: 'Education', 22: 'Automobiles', 23: 'Global Business'}\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = Dataset.from_pandas(df_train)\n",
    "dataset['validation'] = Dataset.from_pandas(df_validation)\n",
    "dataset['test'] = Dataset.from_pandas(df_test)\n",
    "\n",
    "NUM_CLASSES= len(df_train['labels'].unique())\n",
    "\n",
    "TEXT_CLIP='caption'\n",
    "\n",
    "TRAIN_IMAGES_PATH= '../../images/train'\n",
    "VALIDATION_IMAGES_PATH= '../../images/validation'\n",
    "TEST_IMAGES_PATH= '../../images/test'\n",
    "\n",
    "TEXT_TRANSF='text_no_cap'\n",
    "MAX_LENGTH=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CLIP_NAME = 'openai/clip-vit-base-patch32'\n",
    "TRANSFORMER_NAME= 'microsoft/deberta-base'\n",
    "\n",
    "clip_model = AutoModel.from_pretrained(CLIP_NAME)\n",
    "clip_config= AutoConfig.from_pretrained(CLIP_NAME)\n",
    "clip_processor= CLIPProcessor.from_pretrained(CLIP_NAME)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_NAME)\n",
    "config= AutoConfig.from_pretrained(TRANSFORMER_NAME)\n",
    "pretrained_model = AutoModel.from_pretrained(TRANSFORMER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 48180/48180 [02:47<00:00, 287.13ex/s]\n",
      "100%|##########| 6022/6022 [00:20<00:00, 291.76ex/s]\n",
      "100%|##########| 6023/6023 [00:20<00:00, 291.52ex/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    tokens = tokenizer(batch[TEXT_TRANSF], truncation=True, max_length=MAX_LENGTH)\n",
    "    batch['input_ids'], batch['attention_mask'] = tokens['input_ids'], tokens['attention_mask']\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(tokenize)\n",
    "\n",
    "dataset['train'] = dataset['train'].remove_columns(['headline', 'abstract', 'caption', 'image_url', 'article_url', 'image_id', 'body', 'full_text', 'text_no_cap', 'labels_text'])\n",
    "dataset['validation'] = dataset['validation'].remove_columns(['headline', 'abstract', 'caption', 'image_url', 'article_url', 'image_id', 'body', 'full_text', 'text_no_cap', 'labels_text'])\n",
    "dataset['test'] = dataset['test'].remove_columns(['headline', 'abstract', 'caption', 'image_url', 'article_url', 'image_id', 'body', 'full_text', 'text_no_cap', 'labels_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultimodalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, img_dir, ds):\n",
    "        self.df= df\n",
    "        self.img_dir = img_dir\n",
    "        self.ds=ds\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_text= self.df['labels_text'].iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, label_text, self.df['image_id'].iloc[idx])\n",
    "        img_path=img_path + '.jpg'\n",
    "        image = Image.open(img_path)\n",
    "        if(image.mode != 'RGB'):\n",
    "            image=image.convert('RGB')\n",
    "        caption = self.df[TEXT_CLIP].iloc[idx]\n",
    "        text_input_ids= self.ds[idx]['input_ids']\n",
    "        text_attention_mask= self.ds[idx]['attention_mask']\n",
    "        label = self.df['labels'].iloc[idx]\n",
    "        return image, caption, text_input_ids, text_attention_mask, label\n",
    "    \n",
    "train_dataset= CustomMultimodalDataset(df_train, TRAIN_IMAGES_PATH, dataset['train'])\n",
    "validation_dataset= CustomMultimodalDataset(df_validation, VALIDATION_IMAGES_PATH, dataset['validation'])\n",
    "test_dataset= CustomMultimodalDataset(df_test, TEST_IMAGES_PATH, dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalCollator:\n",
    "    HARD_IMG_AUGMENTER = T.RandAugment(num_ops=6, magnitude=9)\n",
    "    SOFT_IMG_AUGMENTER = Compose([T.RandomPerspective(.1, p=.5),\n",
    "                                  T.RandomHorizontalFlip(p=.5),\n",
    "                                ])\n",
    "    \n",
    "    def __init__(self, processor=clip_processor, augment_mode='hard', split='train', max_length=77):\n",
    "        # 40 max length for vilt // 77 max length for clip\n",
    "        self.processor = processor\n",
    "        self.split = split\n",
    "        self.max_length = max_length\n",
    "        self.augment_mode = augment_mode\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, captions, text_input_ids, text_attention_masks, labels = list(zip(*batch))\n",
    "        if self.split=='train' and self.augment_mode == 'hard':\n",
    "            images = [self.HARD_IMG_AUGMENTER(img) for img in images]\n",
    "        elif self.split=='train' and self.augment_mode == 'soft':\n",
    "            images = [self.SOFT_IMG_AUGMENTER(img) for img in images]\n",
    "        \n",
    "        # Pad text_input_ids and text_attention_masks\n",
    "        max_length = max(len(ids) for ids in text_input_ids)\n",
    "        padded_text_input_ids = [ids + [1] * (max_length - len(ids)) for ids in text_input_ids]\n",
    "        padded_text_attention_masks = [masks + [0] * (max_length - len(masks)) for masks in text_attention_masks]\n",
    "\n",
    "        encoding = self.processor(images=images, \n",
    "                                  text=list(captions), \n",
    "                                  padding=True,\n",
    "                                  max_length=self.max_length,\n",
    "                                  truncation=True,\n",
    "                                  return_tensors='pt')\n",
    "        encoding['text_input_ids'] = torch.tensor(padded_text_input_ids)\n",
    "        encoding['text_attention_masks'] = torch.tensor(padded_text_attention_masks)\n",
    "        encoding['labels']=torch.tensor(labels)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "\n",
    "collator_train=MultimodalCollator(split='train')\n",
    "collator_val=MultimodalCollator(split='val')\n",
    "collator_test=MultimodalCollator(split='test')\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collator_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, collate_fn=collator_val, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, collate_fn=collator_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalClassifier(pl.LightningModule):\n",
    "    def __init__(self, clip_model=clip_model, text_transformer= pretrained_model,  lr_transformer=2e-5, lr_heads=2e-3):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lr_transformer=lr_transformer\n",
    "        self.lr_heads=lr_heads\n",
    "        # En el train hacemos media de medias\n",
    "        self.train_loss=[]\n",
    "        self.train_accs=[]\n",
    "        self.train_f1s=[]\n",
    "        \n",
    "        \n",
    "        # Aqui computamos las métricas con todo para mayor precision   \n",
    "        self.val_loss=[]             \n",
    "        self.all_val_y_true=[]\n",
    "        self.all_val_y_pred=[]\n",
    "        \n",
    "        self.text_transformer = text_transformer\n",
    "        self.clip_model = clip_model\n",
    "        # Freeze CLIP model parameters\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # CLIP output\n",
    "        self.clip_fc1 = nn.Linear(1024, 512)\n",
    "        self.clip_activation1 = nn.GELU()\n",
    "        self.output_clip = nn.Linear(256, NUM_CLASSES)\n",
    "        \n",
    "        # Transformer output\n",
    "        self.transformer_fc1 = nn.Linear(768, 512)\n",
    "        self.transformer_activation1 = nn.GELU()\n",
    "        self.output_transformer = nn.Linear(512, NUM_CLASSES)\n",
    "        \n",
    "        # Se puede probar a concatenar el output de CLIP (512) con una proyección del output del transformer \n",
    "        # (768 proyectarlo a 512)\n",
    "        \n",
    "        self.fusion_fc1 = nn.Linear(1024, 512)\n",
    "        self.fusion_activation1 = nn.GELU()\n",
    "        self.fusion_output = nn.Linear(512, NUM_CLASSES)\n",
    "        \n",
    "    def compute_outputs(self, input_ids, attention_mask, pixel_values, text_input_ids, text_attention_masks):\n",
    "        # Get CLIP embedding\n",
    "        out_text=self.clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out_image=self.clip_model.get_image_features(pixel_values=pixel_values)\n",
    "        \n",
    "        combined_embed= torch.cat((out_text,out_image), dim=-1) # Concat\n",
    "        \n",
    "        clip_embed = self.clip_activation1(self.clip_fc1(combined_embed))\n",
    "        \n",
    "        output_3= self.output_clip(clip_embed)\n",
    "        \n",
    "        # Transformer embedding\n",
    "        \n",
    "        outputs = self.text_transformer(input_ids=text_input_ids, attention_mask=text_attention_masks)\n",
    "        logits = outputs['last_hidden_state'][:, 0]  #Get the CLS tokens (deberta)\n",
    "        # logits = outputs.pooler_output\n",
    "        transformer_embed = self.transformer_activation1(self.transformer_fc1(logits))\n",
    "        \n",
    "        output_1= self.output_clip(transformer_embed)\n",
    "        \n",
    "        \n",
    "        # Combine Transformer and CLIP and get output\n",
    "        \n",
    "        fusion_embed= torch.cat((transformer_embed,clip_embed), dim=-1) # Concat\n",
    "        \n",
    "        x = self.fusion_activation1(self.fusion_fc1(fusion_embed))\n",
    "        \n",
    "        return output_1, self.fusion_output(x), output_3\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        text_input_ids= batch['text_input_ids']\n",
    "        text_attention_masks=batch['text_attention_masks']\n",
    "        output_1, output_2, output_3 = self.compute_outputs(input_ids, attention_mask, pixel_values, text_input_ids, text_attention_masks)\n",
    "        return output_1, output_2, output_3\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        text_input_ids= batch['text_input_ids']\n",
    "        text_attention_masks=batch['text_attention_masks']\n",
    "        \n",
    "        labels = batch['labels']\n",
    "        #Compute the output logits\n",
    "        output_1, output_2, output_3 = self.compute_outputs(input_ids, attention_mask, pixel_values, text_input_ids, text_attention_masks)\n",
    "        #Compute metrics\n",
    "        loss_1=self.criterion(output_1,labels)\n",
    "        loss_2=self.criterion(output_2,labels)\n",
    "        loss_3=self.criterion(output_3,labels)\n",
    "        total_loss= loss_1 + loss_2 + loss_3\n",
    "        \n",
    "        \n",
    "        preds_2 = torch.argmax(output_2, dim=-1)\n",
    "        \n",
    "        # Acc and loss only with output 2 (fusion)\n",
    "        acc=accuracy_score(y_true=labels.tolist(), y_pred=preds_2.tolist())\n",
    "        f1=f1_score(y_true=labels.tolist(), y_pred=preds_2.tolist(), average='macro')\n",
    "        self.train_loss.append(total_loss)\n",
    "        self.train_accs.append(acc)\n",
    "        self.train_f1s.append(f1)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # outs is a list of whatever you returned in `validation_step`\n",
    "        mean_loss = sum(self.train_loss)/len(self.train_loss)\n",
    "        mean_acc=sum(self.train_accs)/len(self.train_accs)\n",
    "        mean_f1=sum(self.train_f1s)/len(self.train_f1s)\n",
    "        \n",
    "        self.log(\"train_loss\", mean_loss)\n",
    "        self.log(\"train_acc\", mean_acc)\n",
    "        self.log(\"train_f1\", mean_f1)\n",
    "        \n",
    "        self.train_loss=[]\n",
    "        self.train_accs=[]\n",
    "        self.train_f1s=[]\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        text_input_ids= batch['text_input_ids']\n",
    "        text_attention_masks=batch['text_attention_masks']\n",
    "        labels = batch['labels']\n",
    "        #Compute the output logits\n",
    "        output_1, output_2, output_3 = self.compute_outputs(input_ids, attention_mask, pixel_values, text_input_ids, text_attention_masks)\n",
    "        #Compute metrics\n",
    "        loss_1=self.criterion(output_1,labels)\n",
    "        loss_2=self.criterion(output_2,labels)\n",
    "        loss_3=self.criterion(output_3,labels)\n",
    "        total_loss= loss_1 + loss_2 + loss_3\n",
    "        \n",
    "        \n",
    "        preds_2 = torch.argmax(output_2, dim=-1)\n",
    "        \n",
    "        # Acc and loss only with output 2 (fusion)\n",
    "        acc=accuracy_score(y_true=labels.tolist(), y_pred=preds_2.tolist())\n",
    "        f1=f1_score(y_true=labels.tolist(), y_pred=preds_2.tolist(), average='macro')\n",
    "        self.train_loss.append(total_loss)\n",
    "        self.train_accs.append(acc)\n",
    "        self.train_f1s.append(f1)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # outs is a list of whatever you returned in `validation_step`\n",
    "        mean_loss = sum(self.val_loss)/len(self.val_loss)\n",
    "        \n",
    "        acc= accuracy_score(y_true=self.all_val_y_true, y_pred=self.all_val_y_pred)\n",
    "        f1= f1_score(y_true=self.all_val_y_true, y_pred=self.all_val_y_pred, average='macro')\n",
    "        \n",
    "        self.log(\"val_loss\", mean_loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_f1\", f1)\n",
    "        \n",
    "        self.val_loss=[]\n",
    "        self.all_val_y_true=[]\n",
    "        self.all_val_y_pred=[]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': self.text_transformer.parameters(), 'lr': self.lr_transformer},\n",
    "            {'params': self.clip_fc1.parameters()},\n",
    "            {'params': self.transformer_fc1.parameters()},\n",
    "            {'params': self.fusion_fc1.parameters()},\n",
    "            {'params': self.fusion_output.parameters()},\n",
    "        ],lr=self.lr_heads, amsgrad=True, weight_decay=0.01)\n",
    "        \n",
    "        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=5)\n",
    "        return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    },\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name                    | Type             | Params\n",
      "-------------------------------------------------------------\n",
      "0 | criterion               | CrossEntropyLoss | 0     \n",
      "1 | text_transformer        | DebertaModel     | 138 M \n",
      "2 | clip_model              | CLIPModel        | 151 M \n",
      "3 | clip_fc1                | Linear           | 524 K \n",
      "4 | clip_activation1        | GELU             | 0     \n",
      "5 | transformer_fc1         | Linear           | 393 K \n",
      "6 | transformer_activation1 | GELU             | 0     \n",
      "7 | final_fc1               | Linear           | 524 K \n",
      "8 | final_activation1       | GELU             | 0     \n",
      "9 | final_output            | Linear           | 12.3 K\n",
      "-------------------------------------------------------------\n",
      "140 M     Trainable params\n",
      "151 M     Non-trainable params\n",
      "291 M     Total params\n",
      "1,165.339 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|6         | 421/6023 [08:18<1:50:34,  1.18s/it, v_num=0]"
     ]
    }
   ],
   "source": [
    "experiment_name=f'MultiOutput'\n",
    "# Define the callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "     dirpath='../../model_ckpts/Multimodal/CLIP+Transformer',\n",
    "     filename=experiment_name,\n",
    "     monitor='val_f1', mode='max')\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "early_stopping = EarlyStopping('val_f1', patience=15,mode='max')\n",
    "\n",
    "# instantiate the logger object\n",
    "logger = CSVLogger(save_dir=\"../../logs/Multimodal/CLIP+Transformer\", name=experiment_name)\n",
    " \n",
    "\n",
    "my_model=MultimodalClassifier()\n",
    "trainer=pl.Trainer(accelerator=\"gpu\", devices=[0], deterministic=True, max_epochs=60, logger=logger, precision='16-mixed', accumulate_grad_batches=2,\n",
    "                   callbacks=[lr_monitor, early_stopping, checkpoint_callback])\n",
    "trainer.fit(model=my_model,train_dataloaders=train_loader, val_dataloaders=validation_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
