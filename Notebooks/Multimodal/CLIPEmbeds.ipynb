{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, CLIPProcessor, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from PIL import Image\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "SEED=1234542\n",
    "\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "\n",
    "df_train=pd.read_csv('../../data/splitted/train.csv')\n",
    "df_validation=pd.read_csv('../../data/splitted/validation.csv')\n",
    "df_test=pd.read_csv('../../data/splitted/test.csv')\n",
    "\n",
    "# Remove nan from caption column\n",
    "df_train.fillna(value=\"\", inplace=True)\n",
    "df_validation.fillna(value=\"\", inplace=True)\n",
    "df_test.fillna(value=\"\", inplace=True)\n",
    "\n",
    "\n",
    "NUM_CLASSES= len(df_train['labels'].unique())\n",
    "\n",
    "TEXT_USED='caption'\n",
    "\n",
    "TRAIN_IMAGES_PATH= '../../images/train'\n",
    "VALIDATION_IMAGES_PATH= '../../images/validation'\n",
    "TEST_IMAGES_PATH= '../../images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_NAME = 'openai/clip-vit-base-patch32'\n",
    "\n",
    "pretrained_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "clip_config= AutoConfig.from_pretrained(MODEL_NAME)\n",
    "processor= CLIPProcessor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultimodalDataset(Dataset):\n",
    "    def __init__(self, df, img_dir):\n",
    "        self.df= df\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_text= self.df['labels_text'].iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, label_text, self.df['image_id'].iloc[idx])\n",
    "        img_path=img_path + '.jpg'\n",
    "        image = Image.open(img_path)\n",
    "        if(image.mode != 'RGB'):\n",
    "            image=image.convert('RGB')\n",
    "        text = self.df[TEXT_USED].iloc[idx]\n",
    "        label = self.df['labels'].iloc[idx]\n",
    "        return image, text, label\n",
    "    \n",
    "train_dataset= CustomMultimodalDataset(df_train, TRAIN_IMAGES_PATH)\n",
    "validation_dataset= CustomMultimodalDataset(df_validation, VALIDATION_IMAGES_PATH)\n",
    "test_dataset= CustomMultimodalDataset(df_test, TEST_IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalCollator:\n",
    "    HARD_IMG_AUGMENTER = T.RandAugment(num_ops=6, magnitude=9)\n",
    "    SOFT_IMG_AUGMENTER = Compose([T.RandomPerspective(.1, p=.5),\n",
    "                                  T.RandomHorizontalFlip(p=.5),\n",
    "                                ])\n",
    "    \n",
    "    def __init__(self, processor=processor, augment_mode='hard', split='train', max_length=77):\n",
    "        # 40 max length for vilt // 77 max length for clip\n",
    "        self.processor = processor\n",
    "        self.split = split\n",
    "        self.max_length = max_length\n",
    "        self.augment_mode = augment_mode\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, texts, labels = list(zip(*batch))\n",
    "        if self.split=='train' and self.augment_mode == 'hard':\n",
    "            images = [self.HARD_IMG_AUGMENTER(img) for img in images]\n",
    "        elif self.split=='train' and self.augment_mode == 'soft':\n",
    "            images = [self.SOFT_IMG_AUGMENTER(img) for img in images]\n",
    "\n",
    "        encoding = self.processor(images=images, \n",
    "                                  text=list(texts), \n",
    "                                  padding=True,\n",
    "                                  max_length=self.max_length,\n",
    "                                  truncation=True,\n",
    "                                  return_tensors='pt')\n",
    "        encoding['labels']=torch.tensor(labels)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "\n",
    "collator_train=MultimodalCollator(split='train')\n",
    "collator_val=MultimodalCollator(split='val')\n",
    "collator_test=MultimodalCollator(split='test')\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collator_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, prefetch_factor=8, pin_memory=True)\n",
    "validation_loader = DataLoader(validation_dataset, collate_fn=collator_val, batch_size=BATCH_SIZE, num_workers=4, prefetch_factor=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, collate_fn=collator_test, batch_size=BATCH_SIZE,num_workers=4, prefetch_factor=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 8., 3.]) tensor([1., 2., 2.]) tensor([3., 4., 3.]) tensor([3., 2., 3., 2., 4., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "a=torch.Tensor([3,2, 3])\n",
    "b=torch.Tensor([2,4, 1])\n",
    "mul=a*b # multiplicacion element-wise (a1* b1, a2* b2, a3*b3)\n",
    "res=abs(a-b) # resta\n",
    "max=torch.max(a, b) # maximo posiciona posicion\n",
    "\n",
    "concat=torch.cat((a,b), dim=-1) # Concat\n",
    "\n",
    "print(mul, res, max, concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(pl.LightningModule):\n",
    "    def __init__(self, model=pretrained_model,  lr=2e-3):\n",
    "        super(CLIPClassifier, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lr=lr\n",
    "        # En el train hacemos media de medias\n",
    "        self.train_loss=[]\n",
    "        self.train_accs=[]\n",
    "        self.train_f1s=[]\n",
    "        \n",
    "        \n",
    "        # Aqui computamos las m√©tricas con todo para mayor precision   \n",
    "        self.val_loss=[]             \n",
    "        self.all_val_y_true=[]\n",
    "        self.all_val_y_pred=[]\n",
    "        \n",
    "        self.model = model\n",
    "        # Freeze model parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(1024)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.activation1 = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.output = nn.Linear(512, NUM_CLASSES)\n",
    "        \n",
    "    def compute_outputs(self, input_ids, attention_mask, pixel_values):\n",
    "        out_text=self.model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out_image=self.model.get_image_features(pixel_values=pixel_values)\n",
    "        \n",
    "        combined_embed= torch.cat((out_text,out_image), dim=-1) # Concat\n",
    "        x = self.layer_norm(combined_embed)\n",
    "        x = self.activation1(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        x = self.compute_outputs(input_ids, attention_mask, pixel_values)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        #Compute the output logits\n",
    "        logits = self.compute_outputs(input_ids, attention_mask, pixel_values)\n",
    "        #Compute metrics\n",
    "        loss=self.criterion(logits,labels)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc=accuracy_score(y_true=labels.tolist(), y_pred=preds.tolist())\n",
    "        f1=f1_score(y_true=labels.tolist(), y_pred=preds.tolist(), average='macro')\n",
    "        self.train_loss.append(loss)\n",
    "        self.train_accs.append(acc)\n",
    "        self.train_f1s.append(f1)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # outs is a list of whatever you returned in `validation_step`\n",
    "        mean_loss = sum(self.train_loss)/len(self.train_loss)\n",
    "        mean_acc=sum(self.train_accs)/len(self.train_accs)\n",
    "        mean_f1=sum(self.train_f1s)/len(self.train_f1s)\n",
    "        \n",
    "        self.log(\"train_loss\", mean_loss)\n",
    "        self.log(\"train_acc\", mean_acc)\n",
    "        self.log(\"train_f1\", mean_f1)\n",
    "        \n",
    "        self.train_loss=[]\n",
    "        self.train_accs=[]\n",
    "        self.train_f1s=[]\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        #Compute the output logits\n",
    "        logits = self.compute_outputs(input_ids, attention_mask, pixel_values)\n",
    "        #Compute metrics\n",
    "        loss=self.criterion(logits,labels)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        self.val_loss.append(loss)\n",
    "        \n",
    "        self.all_val_y_true.extend(labels.tolist())\n",
    "        self.all_val_y_pred.extend(preds.tolist())\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # outs is a list of whatever you returned in `validation_step`\n",
    "        mean_loss = sum(self.val_loss)/len(self.val_loss)\n",
    "        \n",
    "        acc= accuracy_score(y_true=self.all_val_y_true, y_pred=self.all_val_y_pred)\n",
    "        f1= f1_score(y_true=self.all_val_y_true, y_pred=self.all_val_y_pred, average='macro')\n",
    "        \n",
    "        self.log(\"val_loss\", mean_loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_f1\", f1)\n",
    "        \n",
    "        self.val_loss=[]\n",
    "        self.all_val_y_true=[]\n",
    "        self.all_val_y_pred=[]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr, amsgrad=True, weight_decay=0.01)\n",
    "        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=5)\n",
    "        return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    },\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/nacho/work/model_ckpts/Multimodal/CLIPEmbeds exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | criterion   | CrossEntropyLoss | 0     \n",
      "1 | model       | CLIPModel        | 151 M \n",
      "2 | layer_norm  | LayerNorm        | 2.0 K \n",
      "3 | fc1         | Linear           | 524 K \n",
      "4 | activation1 | GELU             | 0     \n",
      "5 | dropout     | Dropout          | 0     \n",
      "6 | output      | Linear           | 12.3 K\n",
      "-------------------------------------------------\n",
      "539 K     Trainable params\n",
      "151 M     Non-trainable params\n",
      "151 M     Total params\n",
      "607.266   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|3         | 204/6023 [00:22<10:44,  9.03it/s, v_num=0]       "
     ]
    }
   ],
   "source": [
    "experiment_name=f'CLIP_concat_v3'\n",
    "# Define the callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "     dirpath='../../model_ckpts/Multimodal/CLIPEmbeds',\n",
    "     filename=experiment_name,\n",
    "     monitor='val_f1', mode='max')\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "early_stopping = EarlyStopping('val_f1', patience=25,mode='max')\n",
    "\n",
    "# instantiate the logger object\n",
    "logger = CSVLogger(save_dir=\"../../logs/Multimodal/CLIPEmbeds\", name=experiment_name)\n",
    " \n",
    "\n",
    "my_model=CLIPClassifier(pretrained_model)\n",
    "trainer=pl.Trainer(accelerator=\"gpu\", devices=[3], deterministic=True, max_epochs=60, logger=logger, precision='16-mixed', accumulate_grad_batches=2,\n",
    "                   callbacks=[lr_monitor, early_stopping, checkpoint_callback])\n",
    "trainer.fit(model=my_model,train_dataloaders=train_loader, val_dataloaders=validation_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
