{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import CLIPVisionModel, CLIPFeatureExtractor, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from PIL import Image\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "SEED=1234542\n",
    "\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "\n",
    "df_train=pd.read_csv('../../data/splitted/train.csv')\n",
    "df_validation=pd.read_csv('../../data/splitted/validation.csv')\n",
    "df_test=pd.read_csv('../../data/splitted/test.csv')\n",
    "\n",
    "\n",
    "NUM_CLASSES= len(df_train['labels'].unique())\n",
    "\n",
    "TRAIN_IMAGES_PATH= '../../images/train'\n",
    "VALIDATION_IMAGES_PATH= '../../images/validation'\n",
    "TEST_IMAGES_PATH= '../../images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'logit_scale', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_projection.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_NAME = 'openai/clip-vit-base-patch32'\n",
    "\n",
    "pretrained_model = CLIPVisionModel.from_pretrained(MODEL_NAME)\n",
    "config= AutoConfig.from_pretrained(MODEL_NAME)\n",
    "vision_config=config.vision_config\n",
    "image_processor= CLIPFeatureExtractor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_train)):\n",
    "    label_text= df_train['labels_text'].iloc[i]\n",
    "    img_path = os.path.join(TRAIN_IMAGES_PATH, label_text, df_train['image_id'].iloc[i])\n",
    "    img_path=img_path + '.jpg'\n",
    "    image = Image.open(img_path)\n",
    "    if(image.mode!= 'RGB'):\n",
    "       image=image.convert('RGB')\n",
    "       print(f'Converted: {i} {image.mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, img_dir):\n",
    "        self.df= df\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_text= self.df['labels_text'].iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, label_text, self.df['image_id'].iloc[idx])\n",
    "        img_path=img_path + '.jpg'\n",
    "        image = Image.open(img_path)\n",
    "        if(image.mode != 'RGB'):\n",
    "            image=image.convert('RGB')\n",
    "        label = self.df['labels'].iloc[idx]\n",
    "        return image, label\n",
    "    \n",
    "train_dataset= CustomImageDataset(df_train, TRAIN_IMAGES_PATH)\n",
    "validation_dataset= CustomImageDataset(df_validation, VALIDATION_IMAGES_PATH)\n",
    "test_dataset= CustomImageDataset(df_test, TEST_IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionlCollator:\n",
    "    HARD_IMG_AUGMENTER = T.RandAugment(num_ops=6, magnitude=9)\n",
    "    SOFT_IMG_AUGMENTER = Compose([T.RandomPerspective(.1, p=.5),\n",
    "                                  T.RandomHorizontalFlip(p=.5),\n",
    "                                ])\n",
    "    \n",
    "    def __init__(self, processor=image_processor, augment_mode='hard', split='train'):\n",
    "        # 40 max length for vilt // 77 max length for clip\n",
    "        self.processor = processor\n",
    "        self.split = split\n",
    "        self.augment_mode = augment_mode\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, labels = list(zip(*batch))\n",
    "        if self.split=='train' and self.augment_mode == 'hard':\n",
    "            images = [self.HARD_IMG_AUGMENTER(img) for img in images]\n",
    "        elif self.split=='train' and self.augment_mode == 'soft':\n",
    "            images = [self.SOFT_IMG_AUGMENTER(img) for img in images]\n",
    "\n",
    "        encoding = self.processor(images=images, \n",
    "                                  return_tensors='pt')\n",
    "        \n",
    "        encoding['labels']=torch.tensor(labels)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "\n",
    "collator_train=VisionlCollator(split='train')\n",
    "collator_val=VisionlCollator(split='val')\n",
    "collator_test=VisionlCollator(split='test')\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collator_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, collate_fn=collator_val, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, collate_fn=collator_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(pl.LightningModule):\n",
    "    def __init__(self, model=pretrained_model,  lr=2e-5):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lr=lr\n",
    "        # En el train hacemos media de medias\n",
    "        self.train_loss=[]\n",
    "        self.train_accs=[]\n",
    "        self.train_f1s=[]\n",
    "        \n",
    "        \n",
    "        # Aqui computamos las m√©tricas con todo para mayor precision   \n",
    "        self.val_loss=[]             \n",
    "        self.all_val_y_true=[]\n",
    "        self.all_val_y_pred=[]\n",
    "        \n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(vision_config.hidden_size, 512)\n",
    "        self.activation1 = nn.GELU()\n",
    "        self.output = nn.Linear(512, NUM_CLASSES)\n",
    "        \n",
    "    def compute_outputs(self, pixel_values):\n",
    "        outputs = self.model(pixel_values=pixel_values)\n",
    "        logits = outputs.pooler_output\n",
    "        x = self.activation1(self.fc1(logits))\n",
    "        return self.output(x)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        x = self.compute_outputs(pixel_values)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        #Compute the output logits\n",
    "        logits = self.compute_outputs(pixel_values)\n",
    "        #Compute metrics\n",
    "        loss=self.criterion(logits,labels)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc=accuracy_score(y_true=labels.tolist(), y_pred=preds.tolist())\n",
    "        f1=f1_score(y_true=labels.tolist(), y_pred=preds.tolist(), average='macro')\n",
    "        self.train_loss.append(loss)\n",
    "        self.train_accs.append(acc)\n",
    "        self.train_f1s.append(f1)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # outs is a list of whatever you returned in `validation_step`\n",
    "        mean_loss = sum(self.train_loss)/len(self.train_loss)\n",
    "        mean_acc=sum(self.train_accs)/len(self.train_accs)\n",
    "        mean_f1=sum(self.train_f1s)/len(self.train_f1s)\n",
    "        \n",
    "        self.log(\"train_loss\", mean_loss)\n",
    "        self.log(\"train_acc\", mean_acc)\n",
    "        self.log(\"train_f1\", mean_f1)\n",
    "        \n",
    "        self.train_loss=[]\n",
    "        self.train_accs=[]\n",
    "        self.train_f1s=[]\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        #Compute the output logits\n",
    "        logits = self.compute_outputs(pixel_values)\n",
    "        #Compute metrics\n",
    "        loss=self.criterion(logits,labels)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        self.val_loss.append(loss)\n",
    "        \n",
    "        self.all_val_y_true.extend(labels.tolist())\n",
    "        self.all_val_y_pred.extend(preds.tolist())\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # outs is a list of whatever you returned in `validation_step`\n",
    "        mean_loss = sum(self.val_loss)/len(self.val_loss)\n",
    "        \n",
    "        acc= accuracy_score(y_true=self.all_val_y_true, y_pred=self.all_val_y_pred)\n",
    "        f1= f1_score(y_true=self.all_val_y_true, y_pred=self.all_val_y_pred, average='macro')\n",
    "        \n",
    "        self.log(\"val_loss\", mean_loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_f1\", f1)\n",
    "        \n",
    "        self.val_loss=[]\n",
    "        self.all_val_y_true=[]\n",
    "        self.all_val_y_pred=[]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr, amsgrad=True, weight_decay=0.01)\n",
    "        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=5)\n",
    "        return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    },\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | criterion   | CrossEntropyLoss | 0     \n",
      "1 | model       | CLIPVisionModel  | 87.5 M\n",
      "2 | fc1         | Linear           | 393 K \n",
      "3 | activation1 | GELU             | 0     \n",
      "4 | output      | Linear           | 12.3 K\n",
      "-------------------------------------------------\n",
      "87.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.9 M    Total params\n",
      "351.448   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  67%|######7   | 4063/6023 [28:02<13:31,  2.41it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "experiment_name=f'{MODEL_NAME}_only'\n",
    "# Define the callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "     dirpath='../../model_ckpts/Unimodal/Image',\n",
    "     filename=experiment_name,\n",
    "     monitor='val_f1', mode='max')\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "early_stopping = EarlyStopping('val_f1', patience=10,mode='max')\n",
    "\n",
    "# instantiate the logger object\n",
    "logger = CSVLogger(save_dir=\"../../logs/Unimodal/Image\", name=experiment_name)\n",
    " \n",
    "\n",
    "my_model=ImageClassifier(pretrained_model)\n",
    "trainer=pl.Trainer(accelerator=\"gpu\", devices=[0], deterministic=True, max_epochs=40, logger=logger, precision='16-mixed', accumulate_grad_batches=2,\n",
    "                   callbacks=[lr_monitor, early_stopping, checkpoint_callback])\n",
    "trainer.fit(model=my_model,train_dataloaders=train_loader, val_dataloaders=validation_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
