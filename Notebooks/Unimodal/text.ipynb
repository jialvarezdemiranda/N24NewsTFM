{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "SEED=1234542\n",
    "\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "\n",
    "df_train=pd.read_csv('../../data/splitted/train.csv')\n",
    "df_validation=pd.read_csv('../../data/splitted/validation.csv')\n",
    "df_test=pd.read_csv('../../data/splitted/test.csv')\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = Dataset.from_pandas(df_train)\n",
    "dataset['validation'] = Dataset.from_pandas(df_validation)\n",
    "dataset['test'] = Dataset.from_pandas(df_test)\n",
    "\n",
    "NUM_CLASSES= len(df_train['labels'].unique())\n",
    "TEXT_USED='text_no_cap'\n",
    "MAX_LENGTH=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#MODEL_NAME = 'microsoft/deberta-v3-base' # 512 seq length\n",
    "MODEL_NAME = 'allenai/longformer-base-4096' # 4096 seq length\n",
    "# MODEL_NAME = 'mnaylor/mega-base-wikitext' # 2048 seq length\n",
    "#MODEL_NAME='microsoft/deberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "config= AutoConfig.from_pretrained(MODEL_NAME)\n",
    "pretrained_model = AutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 48180/48180 [02:46<00:00, 289.52ex/s]\n",
      "100%|##########| 6022/6022 [00:20<00:00, 294.13ex/s]\n",
      "100%|##########| 6023/6023 [00:20<00:00, 295.33ex/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    tokens = tokenizer(batch[TEXT_USED], truncation=True, max_length=MAX_LENGTH)\n",
    "    batch['input_ids'], batch['attention_mask'] = tokens['input_ids'], tokens['attention_mask']\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(tokenize)\n",
    "\n",
    "dataset['train'] = dataset['train'].remove_columns(['headline', 'abstract', 'caption', 'image_url', 'article_url', 'image_id', 'body', 'full_text', 'text_no_cap', 'labels_text'])\n",
    "dataset['validation'] = dataset['validation'].remove_columns(['headline', 'abstract', 'caption', 'image_url', 'article_url', 'image_id', 'body', 'full_text', 'text_no_cap', 'labels_text'])\n",
    "dataset['test'] = dataset['test'].remove_columns(['headline', 'abstract', 'caption', 'image_url', 'article_url', 'image_id', 'body', 'full_text', 'text_no_cap', 'labels_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "train_loader = DataLoader(dataset['train'], batch_size=BATCH_SIZE, collate_fn=data_collator, shuffle=True)\n",
    "validation_loader = DataLoader(dataset['validation'], batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "test_loader = DataLoader(dataset['test'], batch_size=BATCH_SIZE, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 35193,   622,  ...,    31,   427,     2],\n",
      "        [    0,   597,  8831,  ..., 15705,  2380,     2],\n",
      "        [    0, 30913,  1534,  ...,     6,    38,     2],\n",
      "        ...,\n",
      "        [    0,   250, 41802,  ...,     1,     1,     1],\n",
      "        [    0,  3750,  7378,  ...,     1,     1,     1],\n",
      "        [    0,  2264,    18,  ...,     1,     1,     1]])\n",
      "torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch['input_ids'])\n",
    "    print(batch['input_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(pl.LightningModule):\n",
    "    def __init__(self, model=pretrained_model,  lr_transformer=2e-5, lr_head=2e-3):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lr_transformer=lr_transformer\n",
    "        self.lr_head=lr_head\n",
    "        \n",
    "        # En el train hacemos media de medias\n",
    "        self.train_loss=[]\n",
    "        self.train_accs=[]\n",
    "        self.train_f1s=[]\n",
    "        \n",
    "        \n",
    "        # Aqui computamos las mÃ©tricas con todo para mayor precision   \n",
    "        self.val_loss=[]             \n",
    "        self.all_val_y_true=[]\n",
    "        self.all_val_y_pred=[]\n",
    "        \n",
    "        self.model = model\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        self.fc1 = nn.Linear(config.hidden_size, 512)\n",
    "        # self.fc1 = nn.Linear(config.hidden_size, 64) # Mega\n",
    "        self.activation1 = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.output = nn.Linear(512, NUM_CLASSES)\n",
    "        # self.output = nn.Linear(64, NUM_CLASSES) # Mega\n",
    "        \n",
    "    def compute_outputs(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # logits = outputs['last_hidden_state'][:, 0]  #Get the CLS tokens (deberta)\n",
    "        logits = outputs.pooler_output\n",
    "        x = self.layer_norm(logits)\n",
    "        x = self.activation1(self.fc1(x))\n",
    "        x=self.dropout(x)\n",
    "        return self.output(x)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        x = self.compute_outputs(input_ids, attention_mask)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        #Compute the output logits\n",
    "        logits = self.compute_outputs(input_ids, attention_mask)\n",
    "        #Compute metrics\n",
    "        loss=self.criterion(logits,labels)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc=accuracy_score(y_true=labels.tolist(), y_pred=preds.tolist())\n",
    "        f1=f1_score(y_true=labels.tolist(), y_pred=preds.tolist(), average='macro')\n",
    "        self.train_loss.append(loss)\n",
    "        self.train_accs.append(acc)\n",
    "        self.train_f1s.append(f1)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # outs is a list of whatever you returned in `validation_step`\n",
    "        mean_loss = sum(self.train_loss)/len(self.train_loss)\n",
    "        mean_acc=sum(self.train_accs)/len(self.train_accs)\n",
    "        mean_f1=sum(self.train_f1s)/len(self.train_f1s)\n",
    "        \n",
    "        self.log(\"train_loss\", mean_loss)\n",
    "        self.log(\"train_acc\", mean_acc)\n",
    "        self.log(\"train_f1\", mean_f1)\n",
    "        \n",
    "        self.train_loss=[]\n",
    "        self.train_accs=[]\n",
    "        self.train_f1s=[]\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        #Compute the output logits\n",
    "        logits = self.compute_outputs(input_ids, attention_mask)\n",
    "        #Compute metrics\n",
    "        loss=self.criterion(logits,labels)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        self.val_loss.append(loss)\n",
    "        \n",
    "        self.all_val_y_true.extend(labels.tolist())\n",
    "        self.all_val_y_pred.extend(preds.tolist())\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # outs is a list of whatever you returned in `validation_step`\n",
    "        mean_loss = sum(self.val_loss)/len(self.val_loss)\n",
    "        \n",
    "        acc= accuracy_score(y_true=self.all_val_y_true, y_pred=self.all_val_y_pred)\n",
    "        f1= f1_score(y_true=self.all_val_y_true, y_pred=self.all_val_y_pred, average='macro')\n",
    "        \n",
    "        self.log(\"val_loss\", mean_loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_f1\", f1)\n",
    "        \n",
    "        self.val_loss=[]\n",
    "        self.all_val_y_true=[]\n",
    "        self.all_val_y_pred=[]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': self.model.parameters(), 'lr': self.lr_transformer,'amsgrad':True, 'weight_decay':0.01 },\n",
    "            {'params': self.layer_norm.parameters()},\n",
    "            {'params': self.fc1.parameters()},\n",
    "            {'params': self.output.parameters()},\n",
    "        ],lr=self.lr_head, amsgrad=True, weight_decay=0.01)\n",
    "        \n",
    "        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=5)\n",
    "        return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    },\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | criterion   | CrossEntropyLoss | 0     \n",
      "1 | model       | LongformerModel  | 148 M \n",
      "2 | layer_norm  | LayerNorm        | 1.5 K \n",
      "3 | fc1         | Linear           | 393 K \n",
      "4 | activation1 | GELU             | 0     \n",
      "5 | dropout     | Dropout          | 0     \n",
      "6 | output      | Linear           | 12.3 K\n",
      "-------------------------------------------------\n",
      "149 M     Trainable params\n",
      "0         Non-trainable params\n",
      "149 M     Total params\n",
      "596.268   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 39/12045 [00:33<2:52:55,  1.16it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "experiment_name=f'{MODEL_NAME}_{TEXT_USED}_{MAX_LENGTH}+Reg'\n",
    "# Define the callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "     dirpath='../../model_ckpts/Unimodal/Text',\n",
    "     filename=experiment_name,\n",
    "     monitor='val_f1', mode='max')\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "early_stopping = EarlyStopping('val_f1', patience=7,mode='max')\n",
    "\n",
    "# instantiate the logger object\n",
    "logger = CSVLogger(save_dir=\"../../logs/Unimodal/Text\", name=experiment_name)\n",
    " \n",
    "\n",
    "my_model=TextClassifier(pretrained_model)\n",
    "trainer=pl.Trainer(accelerator=\"gpu\", devices=[0], deterministic=True, max_epochs=20, logger=logger, precision='16-mixed', accumulate_grad_batches=8,\n",
    "                   callbacks=[lr_monitor, early_stopping, checkpoint_callback])\n",
    "trainer.fit(model=my_model,train_dataloaders=train_loader, val_dataloaders=validation_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_predicting = DataLoader(dataset['train'], batch_size=BATCH_SIZE, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TextClassifier.load_from_checkpoint(\"../../model_ckpts/Unimodal/Text/allenai/longformer-base-4096_text_no_cap-v1.ckpt\") \n",
    "# model = TextClassifier.load_from_checkpoint(\"../../model_ckpts/Unimodal/Text/microsoft/deberta-v3-base_text_no_cap.ckpt\") \n",
    "model = TextClassifier.load_from_checkpoint(\"../../model_ckpts/Unimodal/Text/mnaylor/mega-base-wikitext_text_no_cap.ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|##########| 753/753 [00:18<00:00, 39.84it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|##########| 753/753 [00:18<00:00, 41.26it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|##########| 6023/6023 [02:20<00:00, 42.90it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(accelerator=\"gpu\", devices=[1], deterministic=True, max_epochs=25, precision=16, accumulate_grad_batches=8)\n",
    "predictions_test = trainer.predict(model, test_loader)\n",
    "predictions_val = trainer.predict(model, validation_loader)\n",
    "predictions_train = trainer.predict(model, train_loader_predicting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "PICKLE_PATH= '../../Pickles'\n",
    "\n",
    "def get_predictions(loader, predictions, split):\n",
    "    # initialize the variables for storing true and predicted labels\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    # iterate over the batches and compute f1-score and confusion matrix for each batch\n",
    "    for i, batch in enumerate(loader):\n",
    "        preds=torch.argmax(predictions[i], dim=-1)\n",
    "        y_pred, y_true = preds.tolist(), batch['labels'].tolist()\n",
    "\n",
    "        # append the true and predicted labels to the corresponding lists\n",
    "        all_y_true.extend(y_true)\n",
    "        all_y_pred.extend(y_pred)\n",
    "\n",
    "    # compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true=all_y_true, y_pred=all_y_pred)\n",
    "    \n",
    "    with open(f'y_true_{split}.pkl', 'wb') as f1:\n",
    "      pickle.dump(all_y_true, f1)\n",
    "      \n",
    "    with open(f'y_pred_{split}.pkl', 'wb') as f2:\n",
    "      pickle.dump(all_y_pred, f2)\n",
    "      \n",
    "    mean_f1= f1_score(y_true=all_y_true, y_pred=all_y_pred, average='macro')\n",
    "    print(f'Mean F1: {mean_f1}')\n",
    "\n",
    "#get_predictions(train_loader, predictions_train, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1: 0.8647250682555363\n"
     ]
    }
   ],
   "source": [
    "get_predictions(train_loader_predicting, predictions_train, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1: 0.8161034072402145\n"
     ]
    }
   ],
   "source": [
    "get_predictions(validation_loader, predictions_val, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1: 0.807400918652049\n"
     ]
    }
   ],
   "source": [
    "get_predictions(test_loader, predictions_test, split='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
